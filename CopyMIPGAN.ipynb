{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliJavaheriYekta/Enhanced-MIPGAN/blob/master/CopyMIPGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMH0_lGkpoqn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ecb66e8-34e3-4597-ed4e-56ac83e10c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MIPGAN1_Pytorch'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! git clone \"https://github.com/AliJavaheriYekta/MIPGAN1_Pytorch\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy-ncx32sB2x"
      },
      "outputs": [],
      "source": [
        "%cd MIPGAN1_Pytorch/weights/\n",
        "! wget https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r resultedImages -d /content/drive/MyDrive/sharedresults"
      ],
      "metadata": {
        "id": "meJCbMxDo333"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbT4TH1p9_TE",
        "outputId": "c76dd96d-21df-47ce-e1c5-43f07ef1f917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weUJHJveYG1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6187aec2-8b61-4449-ce34-4c8c60f5b913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MIPGAN1_Pytorch\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/MIPGAN1_Pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqQVmqc-dS-f",
        "outputId": "cdaa8428-24c8-4148-eeb6-049c24bfc676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompareImages.py\t outputs\t      source_image\n",
            "comparePlots.py\t\t perceptual_model.py  stylegan_layers.py\n",
            "dnnlib\t\t\t __pycache__\t      TreeConnect.py\n",
            "HistogramComparision.py  README.md\t      weights\n",
            "LossCalculation.py\t resultedImages\n",
            "MIPGAN.ipynb\t\t save_result\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function \n",
        "from __future__ import division\n",
        "!pip install piq\n",
        "from piq import MultiScaleSSIMLoss, FSIMLoss, MDSILoss, PieAPP\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "#import copy\n",
        "from stylegan_layers import  G_mapping,G_synthesis\n",
        "from perceptual_model import VGG16_for_Perceptual\n",
        "from TreeConnect import TreeConnect\n",
        "from LossCalculation import Losses\n",
        "from torchvision.utils import save_image\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fnsIVRoq1bX",
        "outputId": "1b8d8614-f587-493e-afcb-7c088dd78cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: piq in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torchvision>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from piq) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.6.1->piq) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.6.1->piq) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.6.1->piq) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision>=0.6.1->piq) (3.10.0.2)\n",
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD3_iZjtoKkp"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def opt2_initializer(device):\n",
        "  weights = []\n",
        "  for i in range(3):\n",
        "    weights.append(torch.tensor(.5, requires_grad=True, device=device))\n",
        "  return weights\n",
        "\n",
        "def opt2_remover(prms_to_update2):\n",
        "  for w in prms_to_update2:\n",
        "    w.detach().cpu()\n",
        " \n",
        "class MyEnsemble(nn.Module):\n",
        "    def __init__(self, modelA, modelB, num_ftrs, layers_conf):\n",
        "        super(MyEnsemble, self).__init__()\n",
        "        self.modelA = modelA\n",
        "        self.modelB = modelB\n",
        "        # Remove last linear layer\n",
        "        self.modelA.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=layers_conf['dim'],  output_dim=512, div_coefs=layers_conf['sparsity']).to('cuda:0')\n",
        "        self.modelB.fc = TreeConnect(input_dim=num_ftrs, hidden_layers_dim=layers_conf['dim'],  output_dim=512, div_coefs=layers_conf['sparsity']).to('cuda:0')\n",
        "      \n",
        "    def weight_init(self):\n",
        "        self.modelA.fc.weight_init()\n",
        "        self.modelB.fc.weight_init()\n",
        "\n",
        "    def forward(self, im1,im2):\n",
        "        x1 = self.modelA(im1.clone())\n",
        "        m1 = x1.detach()  # clone to make sure x is not changed by inplace methods\n",
        "        x2 = self.modelB(im2.clone())\n",
        "        m2 = x2.detach()\n",
        "        x = (x1 + x2) / 2.0\n",
        "        return x, m1, m2\n",
        " \n",
        "def read_image(img_source):\n",
        "    img = np.array(Image.open(img_source).convert(\"RGB\"))\n",
        "    return img\n",
        " \n",
        "def image_preprocess(img_source1, img_source2, device):\n",
        "    img_source1 = transforms.ToTensor()(img_source1).unsqueeze_(0)\n",
        "    img_source2 = transforms.ToTensor()(img_source2).unsqueeze_(0)\n",
        "    return img_source1.to(device, dtype = torch.float), img_source2.to(device, dtype = torch.float)\n",
        " \n",
        "def adjust_lr(optimizer, lr):\n",
        "    for param in optimizer.param_groups:\n",
        "        param['lr'] = lr\n",
        "    return optimizer\n",
        " \n",
        "def seperate_losses(loss):\n",
        "    loss1 = loss[1]\n",
        "    loss2 = loss[2]\n",
        "    loss = loss[0]\n",
        "    return loss, loss1, loss2\n",
        "\n",
        "def train_model(model, perceptual_net, perceptual_weights, g_synthesis, loss_weights, inputs, optimizer, optimizer2, lr, weighted_loss, num_epochs, device):\n",
        "    global identity_loss\n",
        "    identity_loss = 0\n",
        "    data = []\n",
        "    data_per = []\n",
        "    weights = loss_weights\n",
        "    since = time.time()\n",
        "    lr1 = lr\n",
        "    image1 = inputs[0]\n",
        "    image2 = inputs[1]\n",
        "    loss = 0\n",
        "    for epoch in range(num_epochs+1):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        model.train()  # Set model to training mode\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        morph, out1, out2 = model(image1, image2)\n",
        "        identity_loss, identity_diff = Losses.identity_loss_calc(out1, out2)\n",
        " \n",
        "        morph = morph.unsqueeze(1).repeat(1, 18, 1)\n",
        "        synth = g_synthesis(morph)\n",
        "        \n",
        "        img_p1=image1.clone() #Perceptual loss\n",
        "        img_p2=image2.clone()\n",
        "        upsample2d=torch.nn.Upsample(scale_factor=1024/1024, mode='bilinear')\n",
        "        img_p1=upsample2d(img_p1)\n",
        "        img_p2=upsample2d(img_p2)\n",
        "        img_p = [img_p1, img_p2]\n",
        "        \n",
        "        fsim_loss, ms_ssim_loss, perceptual_loss = Losses.caluclate_loss(synth, inputs, perceptual_net, img_p, upsample2d, perceptual_weights, epoch, weighted_loss)\n",
        "\n",
        "        fsim_loss, fsim_loss1, fsim_loss2  = seperate_losses(fsim_loss)\n",
        "        ms_ssim_loss, ms_ssim_loss1, ms_ssim_loss2  = seperate_losses(ms_ssim_loss)\n",
        "        perceptual_loss, perceptual_loss1, perceptual_loss2  = seperate_losses(perceptual_loss)\n",
        "\n",
        "        loss =  0.0002*perceptual_loss + weights['mssim']*ms_ssim_loss+ weights['fsim']*fsim_loss + weights['id_loss']*identity_loss + weights['id_diff']*identity_diff\n",
        "\n",
        "        # backward + optimize only if in training phase\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch%5==0:\n",
        "          optimizer2.step()\n",
        "\n",
        "        detach_losses = []\n",
        "        for l in [loss, ms_ssim_loss, fsim_loss, perceptual_loss, identity_loss, identity_diff]:\n",
        "            detach_losses.append(l.detach().cpu().numpy())\n",
        "\n",
        "        detach_sep_losses = []\n",
        "        for ls in [ms_ssim_loss1, ms_ssim_loss2, fsim_loss1, fsim_loss2, perceptual_loss1, perceptual_loss2]:\n",
        "            detach_sep_losses.append(ls.detach().cpu().numpy())\n",
        "\n",
        "        if epoch%6==0:\n",
        "            lr = lr*0.95\n",
        "            optimizer = adjust_lr(optimizer, lr)\n",
        " \n",
        "        if epoch%10==0 or epoch==num_epochs-1:\n",
        "             print(\"iter{}: loss -- {}, mssim --{}, fsim --{}, percep_loss --{}, identity_loss --{}, identity_diff --{}\".format(epoch,detach_losses[0][0][0],detach_losses[1][0],detach_losses[2][0],detach_losses[3],detach_losses[4][0][0],detach_losses[5][0][0]))\n",
        "             data.append([epoch, detach_losses[0][0][0],detach_losses[1][0],detach_losses[2][0],\n",
        "                          detach_losses[3],detach_losses[4][0][0],detach_losses[5][0][0]])\n",
        "             \n",
        "             data_per.append([epoch, detach_sep_losses[0][0], detach_sep_losses[1][0],\n",
        "                              detach_sep_losses[2][0], detach_sep_losses[3][0],\n",
        "                              detach_sep_losses[4], detach_sep_losses[5]])\n",
        "             \n",
        "             synth = (synth - torch.min(synth))/(torch.max(synth)-torch.min(synth))\n",
        "             save_image(synth.clamp(0,1),\"save_result/{}.png\".format(epoch))\n",
        "             \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    return data, data_per\n",
        " \n",
        "\n",
        "def run(perceptual_net, g_synthesis, loss_weights, layers_conf, img1, img2, lr, num_epochs, cnt, weighted_loss='none'):\n",
        "  device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "  # Create ensemble model\n",
        "  modelA = models.resnet50(pretrained=True).to(device)\n",
        "  modelB = models.resnet50(pretrained=True).to(device)\n",
        "  num_ftrs = modelA.fc.in_features\n",
        "  # Freeze these models\n",
        "  for param in modelA.parameters():\n",
        "      param.requires_grad_(False)\n",
        "  \n",
        "  for param in modelB.parameters():\n",
        "      param.requires_grad_(False)\n",
        "\n",
        "  model = MyEnsemble(modelA, modelB,num_ftrs, layers_conf).to(device) \n",
        "\n",
        "  inputs = [img1.to(device), img2.to(device)]\n",
        "  prms_to_update = []\n",
        "  for name, param in model.named_parameters():\n",
        "      if param.requires_grad == True:\n",
        "          prms_to_update.append(param)\n",
        "  prms_to_update2 = opt2_initializer(device)\n",
        "\n",
        "  data_header = [\"iter\", \"loss\", \"mssim\", \"fsim\", \"perceptual\", \"identity\", \"identity diff\"]\n",
        "  data_header2 = [\"iter\",\"ms-ssim1\", \"ms-ssim2\", \"fsim1\", \"fsim2\", \"perceptual1\", \"perceptual2\"]\n",
        "\n",
        "  optimizer = optim.Adam(prms_to_update, lr=lr, betas=(0.9,0.999))\n",
        "  optimizer2 = optim.Adagrad(prms_to_update2)\n",
        "  \n",
        "  data, data_per = train_model(model, perceptual_net, prms_to_update2, g_synthesis, loss_weights, inputs, optimizer, optimizer2, lr, weighted_loss, num_epochs=num_epochs , device=device)\n",
        "  data = pd.DataFrame(data, columns=data_header)\n",
        "  data_per = pd.DataFrame(data_per, columns=data_header2)\n",
        "  data.to_csv('outputs/output{}.csv'.format(cnt), index=False)\n",
        "  data_per.to_csv('outputs/output_sep{}.csv'.format(cnt), index=False)\n",
        "\n",
        "  opt2_remover(prms_to_update2)\n",
        "  del prms_to_update2\n",
        "  torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reference_path = 'resultedImages/' #@param {type: \"string\"}\n",
        "source =  13 #@param {type:\"number\"}\n",
        "source_folder = 'source_image/{}/'.format(source)\n",
        "source_images = [source_folder+str(source)+'_1.png', source_folder+str(source)+'_2.png']\n"
      ],
      "metadata": {
        "id": "8_tJ7nZJwE8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def varInit():\n",
        "    layers_conf = {'dim': [1024, 512], 'sparsity': [64, 32]}\n",
        "    # We use pretrained torchvision models here\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'   \n",
        "\n",
        "    img1 = read_image(source_images[0])\n",
        "    img2 = read_image(source_images[1])\n",
        "    img1, img2 = image_preprocess(img1, img2, device)\n",
        "\n",
        "    weighted_loss = 'none'\n",
        "\n",
        "    g_synthesis = G_synthesis(resolution=1024)\n",
        "    g_all = nn.Sequential(OrderedDict([\n",
        "            ('g_mapping', G_mapping()),\n",
        "            ('g_synthesis', G_synthesis(resolution=1024))    \n",
        "            ]))\n",
        "    \n",
        "    g_all.load_state_dict(torch.load(\"weights/karras2019stylegan-ffhq-1024x1024.for_g_all.pt\", map_location=device))\n",
        "    g_all.eval()\n",
        "    g_all.to(device)\n",
        "    \n",
        "    perceptual_net = VGG16_for_Perceptual(n_layers=[2,4,9,16],device=device)\n",
        "    \n",
        "    g_synthesis = g_all[1]\n",
        "    g_synthesis.eval()\n",
        "    g_synthesis.to(device)\n",
        "    del g_all\n",
        "    # Number of epochs to train for \n",
        "    num_epochs = 160\n",
        "    \n",
        "    # Observe that all parameters are being optimized\n",
        "    lr = 0.03\n",
        "\n",
        "    # Train and evaluate\n",
        "    farthest_setted = False\n",
        "    farthest_percpt = False\n",
        "    return perceptual_net, g_synthesis, layers_conf, img1, img2, lr, num_epochs, weighted_loss\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  cnt = 1\n",
        "  configs = []\n",
        "\n",
        "  dir_to_zip = 'save_result/'\n",
        "  conf_headers = [\"number of layers\", \"weights count\", \"mssim weight\", \"fsim weight\", \"epoch count\"]\n",
        "  #(0.7,0.3)\n",
        "  #(1.0,0.0),\n",
        "  weight_tuples = {'train': [(1.0,0.0), (0.75,0.25), (0.6,0.4), (1.0,0.25), (0.75,0.5)],\n",
        "                   'test': [(0.75,0.25)]}\n",
        "  \n",
        "  for wms, wfs in weight_tuples['test']:  \n",
        "    weights = {'mssim': wms,\n",
        "              'fsim': wfs,\n",
        "              'id_loss': 10,\n",
        "              'id_diff': 1}\n",
        "    perceptual_net, g_synthesis, layers_conf, img1, img2, lr, num_epochs, weighted_loss = varInit()\n",
        "    configs.append([layers_conf['dim'], layers_conf['sparsity'], weights['mssim'], weights['fsim'], num_epochs])\n",
        "    \n",
        "    run(perceptual_net, g_synthesis, weights, layers_conf, img1, img2, lr, num_epochs, cnt, weighted_loss)  \n",
        "\n",
        "    output_filename = reference_path + '/file{}.zip'.format(cnt)\n",
        "    os.system( \"zip -r {} {}\".format( output_filename , dir_to_zip ) )\n",
        "    cnt = cnt + 1\n",
        "\n",
        "  data = pd.DataFrame(configs, columns=conf_headers)\n",
        "  data.to_csv('outputs/configs.csv', index=False)"
      ],
      "metadata": {
        "id": "suOe1LL3mV4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip results\n",
        "cnt = 0\n",
        "for zipfile in glob.glob(reference_path + \"*.zip\"):\n",
        "  cnt = cnt + 1\n",
        "  file_to_unzip = zipfile\n",
        "  output_dir = reference_path + '/conf{}'.format(cnt)\n",
        "  os.system( \"unzip {} -d {}\".format( file_to_unzip , output_dir ))\n",
        "  ref = reference_path + '/conf{}/content/drive/MyDrive/MIPGAN1_Pytorch/save_result'.format(cnt,cnt)\n",
        "  dest = reference_path + '/conf{}'.format(cnt)\n",
        "  os.system( \"mv {} {}\".format( ref , dest ))\n",
        "  os.system( 'rm -fr {}'.format(reference_path + 'conf{}/content'.format(cnt)))"
      ],
      "metadata": {
        "id": "ydBwSEDXNhtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepface\n",
        "from deepface import DeepFace"
      ],
      "metadata": {
        "id": "o_K4q1JH-6u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from CompareImages import CompareImages\n",
        "cmp = CompareImages()\n",
        "model = DeepFace.build_model('ArcFace')\n",
        "cmp.compare([reference_path],[source_folder],model)\n",
        "os.system( 'rm {}'.format(reference_path + 'ComparisionResults0.csv'))"
      ],
      "metadata": {
        "id": "Ve-VJYzWh-W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from comparePlots import PlotResults\n",
        "\n",
        "PlotResults().plotResults('resultedImages/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Z5Fpvbrn0-yZ",
        "outputId": "04f69fb0-944e-42fe-8205-f576f2d1f421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "qDBHWULn_MV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove folders\n",
        "for i in range(5):\n",
        "  path = 'Results{}plots' #@param ['conf{}', 'Results{}plots'] {type:\"string\"}\n",
        "  folder = reference_path + path.format(i+1)\n",
        "  os.system( 'rm -fr {}'.format( folder))"
      ],
      "metadata": {
        "id": "evOYPyAP1_70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "dir_to_zip = '/content/drive/MyDrive/sharedresults/resultedImages2' #@param {type: \"string\"}\n",
        "output_filename = '/content/drive/MyDrive/sharedresults/results2.zip' #@param {type: \"string\"}\n",
        "delete_dir_after_download = \"No\"  #@param ['Yes', 'No']\n",
        "\n",
        "os.system( \"zip {} {}\".format( output_filename , dir_to_zip ) )"
      ],
      "metadata": {
        "id": "N6DRBCaKVtnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fff683c-1b59-4a84-d238-ff3a0ae28dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CopyMIPGAN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}